# maze-rl-value-qlearning



Comparative study of **Value Iteration** and **Tabular Q-learning** on a deterministic **10 Ã— 10 maze** that forces the agent to visit a **sub-goal** before the final goal. All code, experiments and visualisations are provided in Jupyter notebooks.

---

## ğŸ“ Project contents
| File | Purpose |
|------|---------|
| `Code.ipynb` | Main notebook: environment, algorithms, plots |
| `Extended Version Proj Maze.ipynb` | Second notebook with an alternative sub-goal layout |
| `Report.pdf` | Full write-up of methodology, experiments and results |

---

## ğŸ§  Algorithms implemented
| Algorithm | Key settings |
|-----------|--------------|
| **Value Iteration** | Î³ = 0.95, Î”V threshold = 1 Ã— 10â»â´; exhaustive sweep over 200 stateâ€“action pairs :contentReference[oaicite:0]{index=0} |
| **Tabular Q-learning** | Î³ = 0.95, Î± = 0.10, Îµ = 0.05 (tuned with Optuna for fastest convergence) :contentReference[oaicite:1]{index=1} |

---

## ğŸŒ Environment
* 10 Ã— 10 grid with walls (black), start (yellow), sub-goal (blue) and end goal (green).  
* **Deterministic** transitions; illegal moves leave the agent in place.  
* **Reward scheme**: step âˆ’0.1, wall âˆ’1, sub-goal +1, goal +10, premature-goal âˆ’10. :contentReference[oaicite:2]{index=2}

---

## ğŸš€ Quick start  

```bash
git clone https://github.com/your-username/maze-rl-value-qlearning.git
cd maze-rl-value-qlearning
# create environment (example using conda)
conda create -n maze-rl python=3.10
conda activate maze-rl
pip install numpy matplotlib tqdm optuna jupyter
jupyter lab
# open Code.ipynb and run all cells
```
